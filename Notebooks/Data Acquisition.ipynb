{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "#webscraping\n",
    "import time, os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from fake_useragent import UserAgent \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromedriver = \"/Applications/chromedriver\" # path to the chromedriver executable\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_df(state):\n",
    "    '''\n",
    "    A fucntion for scraping TripAdvisor to get all info for top 30 attractions for one state\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state as a string\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df containing all top attractions and images for a state\n",
    "    '''\n",
    "    URL = \"https://www.tripadvisor.com/Attractions/\"\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    driver.get(URL)\n",
    "    time.sleep(2)  #pause to be sure page has loaded\n",
    "    \n",
    "    #find state with search on main page\n",
    "    search = driver.find_element_by_name('q')\n",
    "    search.send_keys(state + \", United States\"  )\n",
    "    time.sleep(2)\n",
    "    search.send_keys(Keys.DOWN)\n",
    "    if state == 'Washington DC':\n",
    "        search.send_keys(Keys.DOWN)\n",
    "    search.send_keys(Keys.RETURN)\n",
    "\n",
    "    # click Things to Do button\n",
    "    things_to_do_button = driver.find_elements_by_xpath('//*[contains(text() , \"Things to Do\")]')[0]\n",
    "    things_to_do_button.click()\n",
    "    time.sleep(2)\n",
    "    \n",
    "    error = 0\n",
    "    try:\n",
    "        # click second See All button\n",
    "        see_all_button = driver.find_elements_by_xpath('//*[contains(text() , \"See all\")]')[1]\n",
    "        see_all_button.click()\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        error = error + 1\n",
    "    \n",
    "    #check if second window appears, if yes switch back to main window and click see more button\n",
    "    try: \n",
    "        driver.switch_to.window(driver.window_handles[1])\n",
    "\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "\n",
    "        #click See more button\n",
    "        attraction_button = driver.find_elements_by_xpath('//*[contains(text() , \"See more\")]')[0]\n",
    "        attraction_button.click()\n",
    "        \n",
    "        #initalize BeautifulSoup\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, \"lxml\")\n",
    "    \n",
    "    except:\n",
    "        #initalize BeautifulSoup\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, \"lxml\")\n",
    "        \n",
    "    url = \"https://www.tripadvisor.com\"\n",
    "    links = {}\n",
    "    body = soup.find_all('body')[0]\n",
    "    link_list = body.find_all( 'a') #find all links on page\n",
    "    \n",
    "    #loop through range to get numbers 1-30, we want top 30 attractions of each state, then for each number\n",
    "    #loop through links, add attraction name and link to links dictionary if number match outside loop\n",
    "    for x in range(1,31):\n",
    "        find = str(x) + \"[.] \"\n",
    "        for l in link_list:\n",
    "            if re.search(find,l.text):\n",
    "                name = l.text\n",
    "                name = re.sub(r'^.*? ', '', name) #clean name\n",
    "                links[name] = url + l['href']\n",
    "    \n",
    "    driver.quit() #close browser\n",
    "\n",
    "    return get_state_attraction_info(links, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_attraction_info(link_dict, state):\n",
    "    '''\n",
    "    A fucntion for scraping TripAdvisor to get locations and image links for top 30 attractions for one state\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dictionary, key is attraction name and value is link\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df containing all top attractions, locations, and images for a state\n",
    "    '''\n",
    "    df = pd.DataFrame(columns = ['name', 'location', 'state', 'img_num'])\n",
    "    idx = 0\n",
    "    img_link_dict = {}\n",
    "    \n",
    "    ua = UserAgent()\n",
    "    headers = {'user-agent': ua.random}\n",
    "    \n",
    "    #loop through dictionary append info to df\n",
    "    for key, value in link_dict.items():\n",
    "\n",
    "        response = requests.get(value, headers = headers)\n",
    "        page= response.text\n",
    "        soup = BeautifulSoup(page, \"lxml\")\n",
    "        time.sleep(1)  #pause to be sure page has loaded\n",
    "        \n",
    "        #try to find attraction location, return state name if none\n",
    "        try:\n",
    "            body = soup.find('div', {'data-tab' :'TABS_LOCATION'})\n",
    "            divs = body.find_all('span')\n",
    "            address = divs[3].text\n",
    "        except:\n",
    "            address = state\n",
    "            \n",
    "        img_link_list = get_photo_links(value)\n",
    "        img_link_dict[key] = img_link_list\n",
    "        \n",
    "        df = df.append(pd.DataFrame({'name':key, 'location': address,'state'= state , 'img_num' : len(img_link_list)}, \n",
    "                                    index=[idx]), ignore_index=True)\n",
    "        \n",
    "        idx = idx + 1\n",
    "        \n",
    "        #change user agent, chosen randomly every 3 loops\n",
    "        if idx%3 == 0:\n",
    "            headers = {'user-agent': ua.random}\n",
    "    \n",
    "    return (df,img_link_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_photo_links(URL):\n",
    "    '''\n",
    "    A fucntion for scraping TripAdvisor to get all image links in a gallery for a specified attraction\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url of attraction\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list containing all image links for an attraction\n",
    "    '''\n",
    "    driver = webdriver.Chrome(chromedriver)\n",
    "    driver.get(URL)\n",
    "    time.sleep(2)  #pause to be sure page has loaded\n",
    "\n",
    "    try:\n",
    "        #click All photos button\n",
    "        all_photos_button = driver.find_elements_by_xpath('//*[contains(text() , \"All photos\")]')[0]\n",
    "        all_photos_button.click()\n",
    "        time.sleep(2)\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "    error = 0\n",
    "    try:\n",
    "        #click first photo\n",
    "        all_photos_button = driver.find_elements_by_class_name(\"photoGridImg\")[0]\n",
    "        all_photos_button.click()\n",
    "        time.sleep(1)\n",
    "    except:\n",
    "        error = error +1\n",
    "    \n",
    "    #initalize BeautifulSoup\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, \"lxml\")\n",
    "    time.sleep(1)\n",
    "    \n",
    "    images = soup.find_all('img') #find all imgs\n",
    "    \n",
    "    #loop through imgs, check for 2 image types, add to list if it is a TripAdvisor user uploaded photo\n",
    "    img_links = []\n",
    "    errors = 0\n",
    "    for i in images[:-1]:\n",
    "        try:\n",
    "            try:\n",
    "                link = i.attrs['data-lazyurl']\n",
    "                if re.search('photo',link):\n",
    "                    img_links.append(link)\n",
    "            except:\n",
    "                link = i.attrs['src']\n",
    "                if re.search('photo',link):\n",
    "                    img_links.append(link)\n",
    "        except:\n",
    "            errors = errors + 1\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return img_links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get top 30 attarctions for each state in continental US with location and 50+ images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_df = pd.DataFrame(columns = ['name', 'location', 'img_num'])\n",
    "img_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [\"Alabama\",\"Arkansas\", \"Arizona\",\"California\",\"Colorado\",\"Connecticut\", \"Washington DC\", \"Delaware\",\n",
    "          \"Florida\", \"Georgia\", \"Iowa\", \"Idaho\", \"Illinois\", \"Indiana\", \"Kansas\",\"Kentucky\", \"Louisiana\",\n",
    "          \"Massachusetts\", \"Maryland\", \"Maine\", \"Michigan\", \"Minnesota\", \"Missouri\", \"Mississippi\", \"Montana\",\n",
    "          \"North Carolina\", \"North Dakota\", \"Nebraska\", \"New Hampshire\", \"New Jersey\", \"New Mexico\", \"Nevada\",\n",
    "          \"New York\", \"Ohio\", \"Oklahoma\", \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\",\n",
    "          \"Tennessee\",\"Texas\",\"Utah\",\"Virginia\",\"Vermont\",\"Washington\",\"Wisconsin\",\"West Virginia\", \"Wyoming\"]\n",
    "\n",
    "\n",
    "for s in states:\n",
    "    df, images = get_state_df(s)\n",
    "    loc_df = pd.concat([loc_df, df], axis=0, ignore_index=True)\n",
    "    img_dict.update(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1469 entries, 0 to 1468\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   name      1469 non-null   object\n",
      " 1   location  1469 non-null   object\n",
      " 2   img_num   1469 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 34.6+ KB\n"
     ]
    }
   ],
   "source": [
    "loc_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_df.to_pickle('attractions_loc_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_df = pd.DataFrame.from_dict(img_dict,orient='index')\n",
    "img_df.to_pickle('attractions_img_links_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77867"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(img_df.count()) #number of img links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
